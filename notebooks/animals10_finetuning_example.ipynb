{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning DINOv3 on Animals-10 Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune a DINOv3 model on the [Animals-10 dataset](https://www.kaggle.com/datasets/alessiocorrado99/animals10/data) from Kaggle.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- **Classes**: 10 animal categories (dog, cat, horse, spider, butterfly, chicken, sheep, cow, squirrel, elephant)\n",
    "- **Images**: ~26,000 total images\n",
    "- **Format**: ImageFolder structure\n",
    "- **Use Case**: Multi-class animal classification\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Dataset download and exploration\n",
    "2. Data preprocessing and validation\n",
    "3. Configuration generation\n",
    "4. Model training with multiple paradigms\n",
    "5. Evaluation and comparison\n",
    "6. Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up directories\n",
    "data_dir = project_root / \"data\" / \"animals10\"\n",
    "config_dir = project_root / \"configs\"\n",
    "output_dir = project_root / \"outputs\" / \"animals10_experiment\"\n",
    "\n",
    "# Create directories\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Download and Preparation\n",
    "\n",
    "First, we'll download the Animals-10 dataset from Kaggle. You'll need to have the Kaggle API set up:\n",
    "\n",
    "```bash\n",
    "pip install kaggle\n",
    "# Place your kaggle.json in ~/.kaggle/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset using Kaggle API\n",
    "def download_animals10_dataset(data_dir):\n",
    "    \"\"\"Download Animals-10 dataset from Kaggle.\"\"\"\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    if (data_dir / \"raw-img\").exists():\n",
    "        print(\"Dataset already exists!\")\n",
    "        return\n",
    "    \n",
    "    print(\"Downloading Animals-10 dataset from Kaggle...\")\n",
    "    \n",
    "    # Download using Kaggle API\n",
    "    import kaggle\n",
    "    \n",
    "    try:\n",
    "        kaggle.api.dataset_download_files(\n",
    "            'alessiocorrado99/animals10',\n",
    "            path=data_dir,\n",
    "            unzip=True\n",
    "        )\n",
    "        print(\"Download completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Kaggle API error: {e}\")\n",
    "        print(\"Please ensure you have Kaggle API set up correctly.\")\n",
    "        print(\"Alternative: Download manually from https://www.kaggle.com/datasets/alessiocorrado99/animals10/data\")\n",
    "        return\n",
    "    \n",
    "    # List downloaded files\n",
    "    print(\"\\nDataset structure:\")\n",
    "    for item in data_dir.rglob(\"*\"):\n",
    "        if item.is_dir():\n",
    "            print(f\"üìÅ {item.relative_to(data_dir)}\")\n",
    "\n",
    "# Download the dataset\n",
    "download_animals10_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "def explore_dataset(data_path):\n",
    "    \"\"\"Explore the dataset structure and statistics.\"\"\"\n",
    "    \n",
    "    raw_img_path = data_path / \"raw-img\"\n",
    "    \n",
    "    if not raw_img_path.exists():\n",
    "        print(f\"Dataset not found at {raw_img_path}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Dataset Exploration ===\")\n",
    "    \n",
    "    # Count images per class\n",
    "    class_counts = {}\n",
    "    total_images = 0\n",
    "    \n",
    "    for class_dir in raw_img_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            # Count images in class directory\n",
    "            image_count = len(list(class_dir.glob(\"*.jpg\")))\n",
    "            class_counts[class_dir.name] = image_count\n",
    "            total_images += image_count\n",
    "    \n",
    "    print(f\"Total images: {total_images:,}\")\n",
    "    print(f\"Number of classes: {len(class_counts)}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for class_name, count in sorted_classes:\n",
    "        percentage = (count / total_images) * 100\n",
    "        print(f\"  {class_name:<12}: {count:>5,} images ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    return class_counts, total_images\n",
    "\n",
    "class_counts, total_images = explore_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "if class_counts:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(classes, counts, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    plt.title('Animals-10 Dataset: Class Distribution', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Animal Classes', fontsize=12)\n",
    "    plt.ylabel('Number of Images', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    counts_array = np.array(counts)\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Mean images per class: {counts_array.mean():.0f}\")\n",
    "    print(f\"  Std images per class: {counts_array.std():.0f}\")\n",
    "    print(f\"  Min images per class: {counts_array.min():,}\")\n",
    "    print(f\"  Max images per class: {counts_array.max():,}\")\n",
    "    print(f\"  Class balance ratio: {counts_array.min() / counts_array.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few images from each class\n",
    "def visualize_sample_images(data_path, num_samples=2):\n",
    "    \"\"\"Visualize sample images from each class.\"\"\"\n",
    "    \n",
    "    raw_img_path = data_path / \"raw-img\"\n",
    "    \n",
    "    if not raw_img_path.exists():\n",
    "        return\n",
    "    \n",
    "    classes = [d.name for d in raw_img_path.iterdir() if d.is_dir()]\n",
    "    classes.sort()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(classes), num_samples, \n",
    "                            figsize=(num_samples * 3, len(classes) * 2.5))\n",
    "    fig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = raw_img_path / class_name\n",
    "        image_files = list(class_dir.glob(\"*.jpg\"))[:num_samples]\n",
    "        \n",
    "        for j, img_path in enumerate(image_files):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                ax = axes[i, j] if len(classes) > 1 else axes[j]\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f\"{class_name}\\n{img.size[0]}x{img.size[1]}\")\n",
    "                ax.axis('off')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_sample_images(data_dir, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing and Validation\n",
    "\n",
    "Now we'll preprocess the data using our framework's preprocessing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our preprocessing tools\n",
    "from data.preprocessing import ImagePreprocessor\n",
    "from data.validation import DatasetValidator\n",
    "from data.dataset import create_dataset\n",
    "from data.augmentations import create_transforms\n",
    "\n",
    "# Run data validation and quality analysis\n",
    "def analyze_dataset_quality(data_path):\n",
    "    \"\"\"Analyze dataset quality using our framework tools.\"\"\"\n",
    "    \n",
    "    raw_img_path = data_path / \"raw-img\"\n",
    "    \n",
    "    print(\"=== Dataset Quality Analysis ===\")\n",
    "    \n",
    "    # Initialize validator\n",
    "    validator = DatasetValidator()\n",
    "    \n",
    "    # Validate dataset structure\n",
    "    validation_results = validator.validate_imagefolder_dataset(\n",
    "        raw_img_path,\n",
    "        min_samples_per_class=100,  # Minimum 100 samples per class\n",
    "        validate_images=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset valid: {validation_results['is_valid']}\")\n",
    "    print(f\"Total samples: {validation_results['total_samples']:,}\")\n",
    "    print(f\"Number of classes: {validation_results['num_classes']}\")\n",
    "    \n",
    "    if validation_results['corrupted_images']:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(validation_results['corrupted_images'])} corrupted images\")\n",
    "        for img_path in validation_results['corrupted_images'][:5]:  # Show first 5\n",
    "            print(f\"  - {img_path}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No corrupted images found\")\n",
    "    \n",
    "    # Analyze class balance\n",
    "    balance_info = validator.analyze_class_balance(raw_img_path)\n",
    "    print(f\"\\nClass balance ratio: {balance_info['balance_ratio']:.3f}\")\n",
    "    print(f\"Dataset balanced: {balance_info['is_balanced']}\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Run quality analysis\n",
    "validation_results = analyze_dataset_quality(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset using our framework\n",
    "def preprocess_animals10_dataset(input_dir, output_dir):\n",
    "    \"\"\"Preprocess Animals-10 dataset for training.\"\"\"\n",
    "    \n",
    "    raw_img_path = input_dir / \"raw-img\"\n",
    "    processed_path = output_dir / \"processed\"\n",
    "    \n",
    "    if processed_path.exists():\n",
    "        print(\"Processed dataset already exists!\")\n",
    "        return processed_path\n",
    "    \n",
    "    print(\"Preprocessing Animals-10 dataset...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = ImagePreprocessor(\n",
    "        target_size=(224, 224),\n",
    "        resize_method=\"resize_shortest\",\n",
    "        quality_threshold=50,  # Filter low quality images\n",
    "        convert_grayscale_to_rgb=True\n",
    "    )\n",
    "    \n",
    "    processed_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    # Process each class\n",
    "    for class_dir in tqdm(raw_img_path.iterdir(), desc=\"Processing classes\"):\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        output_class_dir = processed_path / class_name\n",
    "        output_class_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Process images in class\n",
    "        image_files = list(class_dir.glob(\"*.jpg\"))\n",
    "        \n",
    "        for img_path in tqdm(image_files, desc=f\"Processing {class_name}\", leave=False):\n",
    "            try:\n",
    "                # Process image\n",
    "                processed_img = preprocessor.process_image(img_path)\n",
    "                \n",
    "                if processed_img is not None:\n",
    "                    # Save processed image\n",
    "                    output_path = output_class_dir / img_path.name\n",
    "                    processed_img.save(output_path, 'JPEG', quality=95)\n",
    "                    total_processed += 1\n",
    "                else:\n",
    "                    total_failed += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                total_failed += 1\n",
    "    \n",
    "    print(f\"\\nPreprocessing completed:\")\n",
    "    print(f\"  Successfully processed: {total_processed:,} images\")\n",
    "    print(f\"  Failed to process: {total_failed:,} images\")\n",
    "    print(f\"  Success rate: {total_processed/(total_processed+total_failed)*100:.1f}%\")\n",
    "    \n",
    "    return processed_path\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_data_path = preprocess_animals10_dataset(data_dir, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val/test sets\n",
    "def split_animals10_dataset(processed_path, output_dir):\n",
    "    \"\"\"Split the processed dataset into train/val/test sets.\"\"\"\n",
    "    \n",
    "    train_dir = output_dir / \"train\"\n",
    "    val_dir = output_dir / \"val\"\n",
    "    test_dir = output_dir / \"test\"\n",
    "    \n",
    "    if train_dir.exists():\n",
    "        print(\"Dataset already split!\")\n",
    "        return train_dir, val_dir, test_dir\n",
    "    \n",
    "    print(\"Splitting dataset into train/val/test sets...\")\n",
    "    \n",
    "    # Create split directories\n",
    "    for split_dir in [train_dir, val_dir, test_dir]:\n",
    "        split_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Split ratios\n",
    "    train_ratio = 0.7  # 70% for training\n",
    "    val_ratio = 0.15   # 15% for validation\n",
    "    test_ratio = 0.15  # 15% for testing\n",
    "    \n",
    "    np.random.seed(42)  # For reproducible splits\n",
    "    \n",
    "    split_stats = {}\n",
    "    \n",
    "    # Process each class\n",
    "    for class_dir in processed_path.iterdir():\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        \n",
    "        # Create class directories in each split\n",
    "        for split_dir in [train_dir, val_dir, test_dir]:\n",
    "            (split_dir / class_name).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get all images and shuffle\n",
    "        image_files = list(class_dir.glob(\"*.jpg\"))\n",
    "        np.random.shuffle(image_files)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n_images = len(image_files)\n",
    "        n_train = int(n_images * train_ratio)\n",
    "        n_val = int(n_images * val_ratio)\n",
    "        \n",
    "        # Split files\n",
    "        train_files = image_files[:n_train]\n",
    "        val_files = image_files[n_train:n_train + n_val]\n",
    "        test_files = image_files[n_train + n_val:]\n",
    "        \n",
    "        # Copy files to respective directories\n",
    "        for files, split_dir in [(train_files, train_dir), (val_files, val_dir), (test_files, test_dir)]:\n",
    "            for img_path in files:\n",
    "                dest_path = split_dir / class_name / img_path.name\n",
    "                shutil.copy2(img_path, dest_path)\n",
    "        \n",
    "        split_stats[class_name] = {\n",
    "            'total': n_images,\n",
    "            'train': len(train_files),\n",
    "            'val': len(val_files),\n",
    "            'test': len(test_files)\n",
    "        }\n",
    "    \n",
    "    # Print split statistics\n",
    "    print(\"\\nDataset split completed:\")\n",
    "    \n",
    "    total_stats = {'train': 0, 'val': 0, 'test': 0}\n",
    "    \n",
    "    for class_name, stats in split_stats.items():\n",
    "        print(f\"  {class_name:<12}: Train={stats['train']:>4}, Val={stats['val']:>3}, Test={stats['test']:>3}\")\n",
    "        for split in total_stats:\n",
    "            total_stats[split] += stats[split]\n",
    "    \n",
    "    print(f\"\\nTotal counts:\")\n",
    "    for split, count in total_stats.items():\n",
    "        percentage = count / sum(total_stats.values()) * 100\n",
    "        print(f\"  {split.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return train_dir, val_dir, test_dir\n",
    "\n",
    "# Split the dataset\n",
    "train_dir, val_dir, test_dir = split_animals10_dataset(processed_data_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration Generation\n",
    "\n",
    "Let's generate different configurations for various training paradigms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate configurations for different training paradigms\n",
    "def generate_animals10_configs():\n",
    "    \"\"\"Generate different configurations for Animals-10 training.\"\"\"\n",
    "    \n",
    "    base_config = {\n",
    "        \"name\": \"animals10_experiment\",\n",
    "        \"description\": \"DINOv3 fine-tuning on Animals-10 dataset\",\n",
    "        \"seed\": 42,\n",
    "        \n",
    "        \"model\": {\n",
    "            \"variant\": \"dinov3_vitb16\",\n",
    "            \"task_type\": \"classification\",\n",
    "            \"num_classes\": 10,\n",
    "            \"pretrained\": True,\n",
    "            \"dropout\": 0.1\n",
    "        },\n",
    "        \n",
    "        \"data\": {\n",
    "            \"image_size\": [224, 224],\n",
    "            \"batch_size\": 32,\n",
    "            \"num_workers\": 4,\n",
    "            \"pin_memory\": True,\n",
    "            \"shuffle_train\": True,\n",
    "            \"train_data_path\": str(train_dir),\n",
    "            \"val_data_path\": str(val_dir)\n",
    "        },\n",
    "        \n",
    "        \"optimizer\": {\n",
    "            \"name\": \"adamw\",\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8\n",
    "        },\n",
    "        \n",
    "        \"scheduler\": {\n",
    "            \"name\": \"cosine_annealing\",\n",
    "            \"min_lr\": 1e-6,\n",
    "            \"warmup_type\": \"linear\"\n",
    "        },\n",
    "        \n",
    "        \"augmentation\": {\n",
    "            \"domain\": \"natural\",\n",
    "            \"train\": {\n",
    "                \"horizontal_flip\": True,\n",
    "                \"color_jitter\": {\n",
    "                    \"brightness\": 0.2,\n",
    "                    \"contrast\": 0.2,\n",
    "                    \"saturation\": 0.2,\n",
    "                    \"hue\": 0.1\n",
    "                },\n",
    "                \"random_rotation\": 15,\n",
    "                \"gaussian_blur\": 0.1\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"resize_shortest\": True,\n",
    "                \"center_crop\": True,\n",
    "                \"normalize\": True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"logging\": {\n",
    "            \"log_interval\": 10,\n",
    "            \"save_interval\": 5,\n",
    "            \"use_tensorboard\": True,\n",
    "            \"use_wandb\": False  # Set to True if you have W&B account\n",
    "        },\n",
    "        \n",
    "        \"evaluation\": {\n",
    "            \"metrics\": [\"accuracy\", \"top5_accuracy\", \"f1_macro\"],\n",
    "            \"save_predictions\": True,\n",
    "            \"visualize_predictions\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Configuration variants\n",
    "    configs = {}\n",
    "    \n",
    "    # 1. Linear Probing (fast training)\n",
    "    linear_probe_config = base_config.copy()\n",
    "    linear_probe_config.update({\n",
    "        \"name\": \"animals10_linear_probe\",\n",
    "        \"training\": {\n",
    "            \"mode\": \"linear_probe\",\n",
    "            \"epochs\": 50,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"warmup_epochs\": 5,\n",
    "            \"freeze_backbone\": True,\n",
    "            \"mixed_precision\": True,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping\": {\n",
    "                \"patience\": 10,\n",
    "                \"min_delta\": 1e-4\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    configs['linear_probe'] = linear_probe_config\n",
    "    \n",
    "    # 2. Full Fine-tuning (best performance)\n",
    "    full_finetune_config = base_config.copy()\n",
    "    full_finetune_config.update({\n",
    "        \"name\": \"animals10_full_finetune\",\n",
    "        \"training\": {\n",
    "            \"mode\": \"full_fine_tune\",\n",
    "            \"epochs\": 30,\n",
    "            \"learning_rate\": 5e-5,\n",
    "            \"weight_decay\": 0.05,\n",
    "            \"warmup_epochs\": 5,\n",
    "            \"freeze_backbone\": False,\n",
    "            \"mixed_precision\": True,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"differential_lr\": {\n",
    "                \"backbone_lr_ratio\": 0.1,\n",
    "                \"head_lr_ratio\": 1.0\n",
    "            },\n",
    "            \"early_stopping\": {\n",
    "                \"patience\": 8,\n",
    "                \"min_delta\": 1e-4\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    configs['full_finetune'] = full_finetune_config\n",
    "    \n",
    "    # 3. Lightweight model (for resource-constrained environments)\n",
    "    lightweight_config = base_config.copy()\n",
    "    lightweight_config.update({\n",
    "        \"name\": \"animals10_lightweight\",\n",
    "        \"model\": {\n",
    "            **base_config[\"model\"],\n",
    "            \"variant\": \"dinov3_vits16\"  # Smaller model\n",
    "        },\n",
    "        \"data\": {\n",
    "            **base_config[\"data\"],\n",
    "            \"batch_size\": 64  # Larger batch for smaller model\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"mode\": \"full_fine_tune\",\n",
    "            \"epochs\": 40,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"warmup_epochs\": 5,\n",
    "            \"freeze_backbone\": False,\n",
    "            \"mixed_precision\": True,\n",
    "            \"gradient_clipping\": 1.0,\n",
    "            \"early_stopping\": {\n",
    "                \"patience\": 10,\n",
    "                \"min_delta\": 1e-4\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    configs['lightweight'] = lightweight_config\n",
    "    \n",
    "    # Save configurations\n",
    "    for name, config in configs.items():\n",
    "        config_path = config_dir / f\"animals10_{name}.yaml\"\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "        \n",
    "        print(f\"Generated config: {config_path}\")\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Generate configurations\n",
    "configs = generate_animals10_configs()\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"\\n=== Configuration Summary ===\")\n",
    "for name, config in configs.items():\n",
    "    training = config['training']\n",
    "    model = config['model']\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Model: {model['variant']}\")\n",
    "    print(f\"  Mode: {training['mode']}\")\n",
    "    print(f\"  Epochs: {training['epochs']}\")\n",
    "    print(f\"  Learning Rate: {training['learning_rate']}\")\n",
    "    print(f\"  Batch Size: {config['data']['batch_size']}\")\n",
    "    print(f\"  Frozen Backbone: {training.get('freeze_backbone', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "\n",
    "Now let's train models using different paradigms. We'll start with linear probing for quick results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "from models.model_factory import create_model\n",
    "from training.trainer import DINOv3Trainer\n",
    "from utils.schemas import ExperimentConfig\n",
    "from utils.logging import setup_logging\n",
    "import logging\n",
    "\n",
    "# Function to train a model with given configuration\n",
    "def train_animals10_model(config_name, config, notebook_mode=True):\n",
    "    \"\"\"Train Animals-10 model with given configuration.\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Training {config_name.upper()} Model ===\")\n",
    "    \n",
    "    # Create experiment config\n",
    "    experiment_config = ExperimentConfig(**config)\n",
    "    \n",
    "    # Setup logging\n",
    "    if notebook_mode:\n",
    "        logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "        logger = logging.getLogger(\"dinov3_training\")\n",
    "    else:\n",
    "        logger = setup_logging(\n",
    "            name=\"dinov3_training\",\n",
    "            level=\"INFO\",\n",
    "            log_dir=output_dir / config_name / \"logs\"\n",
    "        )\n",
    "    \n",
    "    # Create transforms\n",
    "    transform_manager = create_transforms(\n",
    "        domain=experiment_config.augmentation.domain,\n",
    "        image_size=experiment_config.data.image_size,\n",
    "        train_kwargs=experiment_config.augmentation.train,\n",
    "        val_kwargs=experiment_config.augmentation.val\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = create_dataset(\n",
    "        data_path=experiment_config.data.train_data_path,\n",
    "        annotation_format=\"imagefolder\",\n",
    "        transform=transform_manager.get_train_transform(),\n",
    "        cache_images=False\n",
    "    )\n",
    "    \n",
    "    val_dataset = create_dataset(\n",
    "        data_path=experiment_config.data.val_data_path,\n",
    "        annotation_format=\"imagefolder\",\n",
    "        transform=transform_manager.get_val_transform(),\n",
    "        cache_images=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset: {len(train_dataset):,} samples\")\n",
    "    print(f\"Val dataset: {len(val_dataset):,} samples\")\n",
    "    print(f\"Classes: {train_dataset.classes}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=experiment_config.data.batch_size,\n",
    "        shuffle=experiment_config.data.shuffle_train,\n",
    "        num_workers=experiment_config.data.num_workers,\n",
    "        pin_memory=experiment_config.data.pin_memory,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=experiment_config.data.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=experiment_config.data.num_workers,\n",
    "        pin_memory=experiment_config.data.pin_memory,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"Creating {experiment_config.model.variant} model...\")\n",
    "    model = create_model(experiment_config.model)\n",
    "    model.set_training_mode(experiment_config.training.mode)\n",
    "    \n",
    "    # Print model info\n",
    "    model_info = model.get_model_info()\n",
    "    print(f\"Model: {model_info['variant']}\")\n",
    "    print(f\"Parameters: {model_info['total_parameters']:,} total, {model_info['trainable_parameters']:,} trainable\")\n",
    "    print(f\"Model size: {model_info['model_size_mb']:.1f} MB\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = DINOv3Trainer(\n",
    "        model=model,\n",
    "        config=experiment_config.training,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        logger=logger,\n",
    "        experiment_tracker=None  # Disable external tracking for notebook\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    experiment_output_dir = output_dir / config_name\n",
    "    experiment_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    training_results = trainer.train(experiment_output_dir)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n=== Training Results ({config_name}) ===\")\n",
    "    print(f\"Training time: {end_time - start_time}\")\n",
    "    print(f\"Final epoch: {training_results['final_epoch']}\")\n",
    "    if training_results.get('best_metric'):\n",
    "        print(f\"Best validation accuracy: {training_results['best_metric']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'config_name': config_name,\n",
    "        'model': model,\n",
    "        'training_results': training_results,\n",
    "        'experiment_config': experiment_config,\n",
    "        'datasets': {'train': train_dataset, 'val': val_dataset},\n",
    "        'output_dir': experiment_output_dir\n",
    "    }\n",
    "\n",
    "# Train linear probe model (fastest)\n",
    "print(\"Starting with Linear Probe training for quick results...\")\n",
    "linear_probe_results = train_animals10_model('linear_probe', configs['linear_probe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train full fine-tuning model (better performance, takes longer)\n",
    "train_full_finetune = input(\"Train full fine-tuning model? (y/n): \").lower().startswith('y')\n",
    "\n",
    "if train_full_finetune:\n",
    "    print(\"Training full fine-tuning model (this will take longer)...\")\n",
    "    full_finetune_results = train_animals10_model('full_finetune', configs['full_finetune'])\n",
    "else:\n",
    "    print(\"Skipping full fine-tuning (you can run this later)\")\n",
    "    full_finetune_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation and Analysis\n",
    "\n",
    "Let's evaluate our trained model(s) and analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation tools\n",
    "from evaluation.evaluators import create_evaluator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_animals10_model(model_results, test_data_path):\n",
    "    \"\"\"Comprehensive evaluation of trained model.\"\"\"\n",
    "    \n",
    "    config_name = model_results['config_name']\n",
    "    model = model_results['model']\n",
    "    experiment_config = model_results['experiment_config']\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {config_name.upper()} Model ===\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    transform_manager = create_transforms(\n",
    "        domain=experiment_config.augmentation.domain,\n",
    "        image_size=experiment_config.data.image_size\n",
    "    )\n",
    "    \n",
    "    test_dataset = create_dataset(\n",
    "        data_path=str(test_data_path),\n",
    "        annotation_format=\"imagefolder\",\n",
    "        transform=transform_manager.get_val_transform(),\n",
    "        cache_images=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Test dataset: {len(test_dataset):,} samples\")\n",
    "    \n",
    "    # Create evaluator\n",
    "    device = next(model.parameters()).device\n",
    "    evaluator = create_evaluator(\n",
    "        task_type=experiment_config.model.task_type,\n",
    "        num_classes=experiment_config.model.num_classes,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    print(\"Running evaluation...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs['logits'] if isinstance(outputs, dict) else outputs\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            confidences = torch.softmax(logits, dim=1).max(dim=1)[0]\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            \n",
    "            # Update evaluator\n",
    "            evaluator.update(logits.cpu(), targets.cpu())\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = evaluator.compute()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTest Results ({config_name}):\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = test_dataset.classes\n",
    "    print(f\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {config_name.title()} Model')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(class_names, per_class_acc, color='skyblue', edgecolor='navy')\n",
    "    plt.title(f'Per-Class Accuracy - {config_name.title()} Model')\n",
    "    plt.xlabel('Animal Classes')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy labels on bars\n",
    "    for bar, acc in zip(bars, per_class_acc):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confidence analysis\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(all_confidences, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Confidence Score Distribution')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    correct_predictions = np.array(all_predictions) == np.array(all_targets)\n",
    "    correct_confidences = np.array(all_confidences)[correct_predictions]\n",
    "    incorrect_confidences = np.array(all_confidences)[~correct_predictions]\n",
    "    \n",
    "    plt.hist(correct_confidences, bins=30, alpha=0.7, label='Correct', color='green')\n",
    "    plt.hist(incorrect_confidences, bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Confidence: Correct vs Incorrect')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'test_metrics': metrics,\n",
    "        'per_class_accuracy': dict(zip(class_names, per_class_acc)),\n",
    "        'classification_report': classification_report(all_targets, all_predictions, target_names=class_names, output_dict=True),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'confidence_stats': {\n",
    "            'mean': float(np.mean(all_confidences)),\n",
    "            'std': float(np.std(all_confidences)),\n",
    "            'mean_correct': float(np.mean(correct_confidences)),\n",
    "            'mean_incorrect': float(np.mean(incorrect_confidences))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    results_path = model_results['output_dir'] / 'test_results.json'\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate the trained model(s)\n",
    "linear_probe_eval = evaluate_animals10_model(linear_probe_results, test_dir)\n",
    "\n",
    "if 'full_finetune_results' in locals() and full_finetune_results is not None:\n",
    "    full_finetune_eval = evaluate_animals10_model(full_finetune_results, test_dir)\n",
    "else:\n",
    "    full_finetune_eval = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models (if we have multiple)\n",
    "if full_finetune_eval is not None:\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    \n",
    "    models_comparison = {\n",
    "        'Linear Probe': linear_probe_eval['test_metrics'],\n",
    "        'Full Fine-tune': full_finetune_eval['test_metrics']\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(models_comparison).round(4)\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualization\n",
    "    metrics_to_compare = ['accuracy', 'f1_macro']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metrics_to_compare), figsize=(15, 5))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_to_compare):\n",
    "        values = [models_comparison[model][metric] for model in models_comparison.keys()]\n",
    "        \n",
    "        ax = axes[idx] if len(metrics_to_compare) > 1 else axes\n",
    "        bars = ax.bar(models_comparison.keys(), values, \n",
    "                     color=['lightcoral', 'skyblue'], edgecolor='navy')\n",
    "        ax.set_title(f'{metric.title()} Comparison')\n",
    "        ax.set_ylabel(metric.title())\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nOnly linear probe model was trained. To compare with full fine-tuning, set train_full_finetune=True above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Deployment\n",
    "\n",
    "Let's deploy our best model as an API server for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "def get_best_model_path(results_list):\n",
    "    \"\"\"Find the path to the best trained model.\"\"\"\n",
    "    \n",
    "    available_models = []\n",
    "    \n",
    "    if linear_probe_results:\n",
    "        model_path = linear_probe_results['output_dir'] / 'best_model.pth'\n",
    "        if model_path.exists():\n",
    "            available_models.append({\n",
    "                'name': 'Linear Probe',\n",
    "                'path': model_path,\n",
    "                'config': linear_probe_results['output_dir'] / 'config.yaml',\n",
    "                'accuracy': linear_probe_eval['test_metrics']['accuracy']\n",
    "            })\n",
    "    \n",
    "    if full_finetune_eval is not None:\n",
    "        model_path = full_finetune_results['output_dir'] / 'best_model.pth'\n",
    "        if model_path.exists():\n",
    "            available_models.append({\n",
    "                'name': 'Full Fine-tune',\n",
    "                'path': model_path,\n",
    "                'config': full_finetune_results['output_dir'] / 'config.yaml',\n",
    "                'accuracy': full_finetune_eval['test_metrics']['accuracy']\n",
    "            })\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"No trained models found!\")\n",
    "        return None\n",
    "    \n",
    "    # Select best model\n",
    "    best_model = max(available_models, key=lambda x: x['accuracy'])\n",
    "    \n",
    "    print(f\"Best model: {best_model['name']} (Accuracy: {best_model['accuracy']:.4f})\")\n",
    "    print(f\"Model path: {best_model['path']}\")\n",
    "    print(f\"Config path: {best_model['config']}\")\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "best_model_info = get_best_model_path([linear_probe_results, full_finetune_results if 'full_finetune_results' in locals() else None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with the best model\n",
    "def test_model_inference(model_path, config_path, test_images_dir):\n",
    "    \"\"\"Test inference with a few sample images.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Testing Model Inference ===\")\n",
    "    \n",
    "    # Load model and config (this simulates what the API server does)\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    config_dict = checkpoint['config']\n",
    "    experiment_config = ExperimentConfig(**config_dict)\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(experiment_config.model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Create transforms\n",
    "    transform_manager = create_transforms(\n",
    "        domain=experiment_config.augmentation.domain,\n",
    "        image_size=experiment_config.data.image_size\n",
    "    )\n",
    "    val_transform = transform_manager.get_val_transform()\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = checkpoint.get('class_names', [f'class_{i}' for i in range(experiment_config.model.num_classes)])\n",
    "    \n",
    "    # Test on a few random images\n",
    "    test_results = []\n",
    "    \n",
    "    # Collect sample images from test set\n",
    "    sample_images = []\n",
    "    for class_dir in test_images_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            class_images = list(class_dir.glob(\"*.jpg\"))[:2]  # 2 images per class\n",
    "            for img_path in class_images:\n",
    "                sample_images.append((img_path, class_dir.name))\n",
    "    \n",
    "    # Randomly select 10 images\n",
    "    np.random.shuffle(sample_images)\n",
    "    sample_images = sample_images[:10]\n",
    "    \n",
    "    print(f\"Testing inference on {len(sample_images)} sample images...\")\n",
    "    \n",
    "    # Create a visualization\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img_path, true_class) in enumerate(sample_images):\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            input_tensor = val_transform(image).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            start_time = time.time()\n",
    "            outputs = model(input_tensor)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Get prediction\n",
    "            logits = outputs['logits'] if isinstance(outputs, dict) else outputs\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "            \n",
    "            predicted_class = class_names[predicted_idx.item()]\n",
    "            confidence_score = confidence.item()\n",
    "            \n",
    "            # Store results\n",
    "            test_results.append({\n",
    "                'image_path': str(img_path),\n",
    "                'true_class': true_class,\n",
    "                'predicted_class': predicted_class,\n",
    "                'confidence': confidence_score,\n",
    "                'correct': predicted_class == true_class,\n",
    "                'inference_time_ms': inference_time * 1000\n",
    "            })\n",
    "            \n",
    "            # Visualize\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(image)\n",
    "            \n",
    "            # Color code: green for correct, red for incorrect\n",
    "            color = 'green' if predicted_class == true_class else 'red'\n",
    "            \n",
    "            ax.set_title(f'True: {true_class}\\nPred: {predicted_class}\\nConf: {confidence_score:.3f}\\n{inference_time*1000:.1f}ms',\n",
    "                        fontsize=10, color=color, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Inference Results', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    correct_predictions = sum(1 for r in test_results if r['correct'])\n",
    "    avg_confidence = np.mean([r['confidence'] for r in test_results])\n",
    "    avg_inference_time = np.mean([r['inference_time_ms'] for r in test_results])\n",
    "    \n",
    "    print(f\"\\nInference Test Summary:\")\n",
    "    print(f\"  Accuracy on samples: {correct_predictions}/{len(test_results)} ({correct_predictions/len(test_results)*100:.1f}%)\")\n",
    "    print(f\"  Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"  Average inference time: {avg_inference_time:.1f} ms\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Test inference\n",
    "if best_model_info:\n",
    "    inference_results = test_model_inference(\n",
    "        best_model_info['path'],\n",
    "        best_model_info['config'],\n",
    "        test_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for deploying the model as an API\n",
    "if best_model_info:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ DEPLOYMENT INSTRUCTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. Start the API server:\")\n",
    "    print(f\"   MODEL_PATH='{best_model_info['path']}' \\\\\")\n",
    "    print(f\"   CONFIG_PATH='{best_model_info['config']}' \\\\\")\n",
    "    print(f\"   python {project_root}/src/api/server.py\")\n",
    "    \n",
    "    print(\"\\n2. Test the API:\")\n",
    "    print('   curl -X POST \"http://localhost:8000/predict\" \\\\')\n",
    "    print('     -F \"file=@path/to/your/animal/image.jpg\" \\\\')\n",
    "    print('     -F \"confidence_threshold=0.5\"')\n",
    "    \n",
    "    print(\"\\n3. View API documentation:\")\n",
    "    print('   Open: http://localhost:8000/docs')\n",
    "    \n",
    "    print(\"\\n4. Docker deployment:\")\n",
    "    print(f\"   docker build -f {project_root}/docker/Dockerfile.deploy -t animals10-api .\")\n",
    "    print(f\"   docker run -p 8000:8000 -v {best_model_info['path'].parent}:/models animals10-api\")\n",
    "    \n",
    "    print(\"\\n5. Kubernetes deployment:\")\n",
    "    print(f\"   kubectl apply -f {project_root}/k8s/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Save deployment config\n",
    "    deployment_config = {\n",
    "        'model_name': 'Animals-10 Classifier',\n",
    "        'model_path': str(best_model_info['path']),\n",
    "        'config_path': str(best_model_info['config']),\n",
    "        'model_type': best_model_info['name'],\n",
    "        'test_accuracy': best_model_info['accuracy'],\n",
    "        'classes': [\n",
    "            'butterfly', 'cat', 'chicken', 'cow', 'dog',\n",
    "            'elephant', 'horse', 'sheep', 'spider', 'squirrel'\n",
    "        ],\n",
    "        'deployment_date': datetime.now().isoformat(),\n",
    "        'performance_metrics': {\n",
    "            'avg_inference_time_ms': np.mean([r['inference_time_ms'] for r in inference_results]),\n",
    "            'avg_confidence': np.mean([r['confidence'] for r in inference_results])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    deployment_config_path = output_dir / 'deployment_config.json'\n",
    "    with open(deployment_config_path, 'w') as f:\n",
    "        json.dump(deployment_config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDeployment configuration saved to: {deployment_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ **Downloaded and explored** the Animals-10 dataset (26k+ images, 10 classes)\n",
    "2. ‚úÖ **Preprocessed and validated** the data using our framework tools\n",
    "3. ‚úÖ **Generated configurations** for different training paradigms\n",
    "4. ‚úÖ **Trained DINOv3 models** using linear probing (and optionally full fine-tuning)\n",
    "5. ‚úÖ **Evaluated model performance** with comprehensive metrics and visualizations\n",
    "6. ‚úÖ **Tested inference** on sample images with timing analysis\n",
    "7. ‚úÖ **Prepared for deployment** with API server instructions\n",
    "\n",
    "**Key Results:**\n",
    "- **Dataset**: 10 animal classes with good class balance\n",
    "- **Model Performance**: Achieved high accuracy on animal classification\n",
    "- **Inference Speed**: Fast inference suitable for real-time applications\n",
    "- **Deployment Ready**: Model ready for production deployment\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. **üöÄ Deploy the API**: Follow the deployment instructions above\n",
    "2. **üì± Build an App**: Create a web or mobile app using the API\n",
    "3. **üéØ Fine-tune Further**: Try different hyperparameters or model variants\n",
    "4. **üìä Monitor Performance**: Set up monitoring in production\n",
    "5. **üîÑ Continuous Learning**: Implement feedback loops for model improvement\n",
    "\n",
    "**Additional Experiments to Try:**\n",
    "\n",
    "- **Different Model Variants**: Try ViT-L/16 or ViT-S/16\n",
    "- **Data Augmentation**: Experiment with domain-specific augmentations\n",
    "- **Model Optimization**: Apply quantization or pruning for faster inference\n",
    "- **Multi-Label Extension**: Modify for multi-label animal detection\n",
    "- **Transfer to Related Tasks**: Use the trained features for other animal datasets\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've successfully fine-tuned a DINOv3 model for animal classification and prepared it for deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}